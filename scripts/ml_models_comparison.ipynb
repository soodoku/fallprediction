{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive ML Models for Fall Prediction\n",
    "\n",
    "This notebook implements multiple machine learning models for fall prediction:\n",
    "- **Neural Network** (Multi-layer Perceptron)\n",
    "- **Support Vector Machine** (SVM)\n",
    "- **Random Forest** with OOB error\n",
    "- **Gradient Boosting**\n",
    "- **XGBoost**\n",
    "- **Logistic Regression**\n",
    "\n",
    "All models use:\n",
    "- 75/25 stratified train/test split\n",
    "- Hyperparameter search (GridSearchCV or RandomizedSearchCV)\n",
    "- OOB error where applicable (Random Forest, Gradient Boosting)\n",
    "\n",
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    classification_report, f1_score, precision_score, recall_score\n",
    ")\n",
    "\n",
    "# Models\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../data/combined_output.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nClass distribution:\\n{df['Faller'].value_counts()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing = df.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"No missing values\")\n",
    "\n",
    "# Handle missing values with median imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "feature_cols = [col for col in df.columns if col not in ['ID', 'Faller']]\n",
    "df[feature_cols] = imputer.fit_transform(df[feature_cols])\n",
    "\n",
    "print(f\"\\nAfter imputation - Missing values: {df.isnull().sum().sum()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation: 75/25 Stratified Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df[feature_cols].values\n",
    "y = (df['Faller'] == 'F').astype(int).values  # Convert to binary: F=1, NF=0\n",
    "\n",
    "# 75/25 stratified train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(f\"  Non-fallers (0): {np.sum(y_train == 0)} ({np.sum(y_train == 0)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"  Fallers (1): {np.sum(y_train == 1)} ({np.sum(y_train == 1)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(f\"  Non-fallers (0): {np.sum(y_test == 0)} ({np.sum(y_test == 0)/len(y_test)*100:.1f}%)\")\n",
    "print(f\"  Fallers (1): {np.sum(y_test == 1)} ({np.sum(y_test == 1)/len(y_test)*100:.1f}%)\")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nNumber of features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_proba=None):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive metrics for binary classification.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred) * 100,\n",
    "        'sensitivity': tp / (tp + fn) * 100 if (tp + fn) > 0 else 0,\n",
    "        'specificity': tn / (tn + fp) * 100 if (tn + fp) > 0 else 0,\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0) * 100,\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0) * 100,\n",
    "    }\n",
    "    \n",
    "    if y_proba is not None:\n",
    "        metrics['auc'] = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def print_metrics(metrics, title):\n",
    "    \"\"\"\n",
    "    Print metrics in a formatted way.\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Accuracy:    {metrics['accuracy']:.2f}%\")\n",
    "    print(f\"Sensitivity: {metrics['sensitivity']:.2f}%\")\n",
    "    print(f\"Specificity: {metrics['specificity']:.2f}%\")\n",
    "    print(f\"Precision:   {metrics['precision']:.2f}%\")\n",
    "    print(f\"F1 Score:    {metrics['f1']:.2f}%\")\n",
    "    if 'auc' in metrics:\n",
    "        print(f\"AUC:         {metrics['auc']:.4f}\")\n",
    "    if 'oob_score' in metrics:\n",
    "        print(f\"OOB Score:   {metrics['oob_score']:.2f}%\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Neural Network (Multi-layer Perceptron)\n",
    "\n",
    "Hyperparameter search for:\n",
    "- Hidden layer sizes\n",
    "- Activation functions\n",
    "- Learning rate\n",
    "- Alpha (L2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"MODEL 1: NEURAL NETWORK (Multi-layer Perceptron)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "nn_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate_init': [0.001, 0.01],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "\n",
    "print(f\"\\nHyperparameter search space:\")\n",
    "print(f\"  Hidden layer sizes: {nn_param_grid['hidden_layer_sizes']}\")\n",
    "print(f\"  Activation functions: {nn_param_grid['activation']}\")\n",
    "print(f\"  Alpha (L2): {nn_param_grid['alpha']}\")\n",
    "print(f\"  Learning rates: {nn_param_grid['learning_rate_init']}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in nn_param_grid.values()])}\")\n",
    "\n",
    "# GridSearchCV with 5-fold cross-validation\n",
    "nn_grid = GridSearchCV(\n",
    "    MLPClassifier(random_state=RANDOM_STATE, early_stopping=True),\n",
    "    nn_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nRunning GridSearchCV (this may take a few minutes)...\")\n",
    "nn_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {nn_grid.best_params_}\")\n",
    "print(f\"Best CV accuracy: {nn_grid.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "nn_best = nn_grid.best_estimator_\n",
    "y_pred_nn = nn_best.predict(X_test_scaled)\n",
    "y_proba_nn = nn_best.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "nn_metrics = calculate_metrics(y_test, y_pred_nn, y_proba_nn)\n",
    "print_metrics(nn_metrics, \"Neural Network - Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Support Vector Machine (SVM)\n",
    "\n",
    "Hyperparameter search for:\n",
    "- Kernel types (RBF, linear, polynomial)\n",
    "- C (regularization parameter)\n",
    "- Gamma (kernel coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 2: SUPPORT VECTOR MACHINE (SVM)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "svm_param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['rbf', 'linear', 'poly'],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "    'probability': [True]  # For AUC calculation\n",
    "}\n",
    "\n",
    "print(f\"\\nHyperparameter search space:\")\n",
    "print(f\"  C: {svm_param_grid['C']}\")\n",
    "print(f\"  Kernels: {svm_param_grid['kernel']}\")\n",
    "print(f\"  Gamma: {svm_param_grid['gamma']}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) if isinstance(v, list) else 1 for v in svm_param_grid.values()])}\")\n",
    "\n",
    "# GridSearchCV with 5-fold cross-validation\n",
    "svm_grid = GridSearchCV(\n",
    "    SVC(random_state=RANDOM_STATE),\n",
    "    svm_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nRunning GridSearchCV (this may take a few minutes)...\")\n",
    "svm_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {svm_grid.best_params_}\")\n",
    "print(f\"Best CV accuracy: {svm_grid.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "svm_best = svm_grid.best_estimator_\n",
    "y_pred_svm = svm_best.predict(X_test_scaled)\n",
    "y_proba_svm = svm_best.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "svm_metrics = calculate_metrics(y_test, y_pred_svm, y_proba_svm)\n",
    "print_metrics(svm_metrics, \"SVM - Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Random Forest with OOB Error\n",
    "\n",
    "Hyperparameter search for:\n",
    "- Number of estimators\n",
    "- Max features\n",
    "- Max depth\n",
    "- Min samples split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 3: RANDOM FOREST with OOB Error\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 365, 500],\n",
    "    'max_features': [1, 'sqrt', 'log2'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'oob_score': [True]\n",
    "}\n",
    "\n",
    "print(f\"\\nHyperparameter search space:\")\n",
    "print(f\"  N estimators: {rf_param_grid['n_estimators']}\")\n",
    "print(f\"  Max features: {rf_param_grid['max_features']}\")\n",
    "print(f\"  Max depth: {rf_param_grid['max_depth']}\")\n",
    "print(f\"  Min samples split: {rf_param_grid['min_samples_split']}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) if isinstance(v, list) else 1 for v in rf_param_grid.values()])}\")\n",
    "\n",
    "# RandomizedSearchCV for efficiency (too many combinations)\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestClassifier(random_state=RANDOM_STATE),\n",
    "    rf_param_grid,\n",
    "    n_iter=50,  # Try 50 random combinations\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\nRunning RandomizedSearchCV (this may take a few minutes)...\")\n",
    "rf_random.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {rf_random.best_params_}\")\n",
    "print(f\"Best CV accuracy: {rf_random.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "rf_best = rf_random.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_test_scaled)\n",
    "y_proba_rf = rf_best.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "rf_metrics = calculate_metrics(y_test, y_pred_rf, y_proba_rf)\n",
    "rf_metrics['oob_score'] = rf_best.oob_score_ * 100\n",
    "print_metrics(rf_metrics, \"Random Forest - Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Gradient Boosting with OOB Improvement\n",
    "\n",
    "Hyperparameter search for:\n",
    "- Number of estimators\n",
    "- Learning rate\n",
    "- Max depth\n",
    "- Subsample ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 4: GRADIENT BOOSTING with OOB Improvement\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "gb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "print(f\"\\nHyperparameter search space:\")\n",
    "print(f\"  N estimators: {gb_param_grid['n_estimators']}\")\n",
    "print(f\"  Learning rate: {gb_param_grid['learning_rate']}\")\n",
    "print(f\"  Max depth: {gb_param_grid['max_depth']}\")\n",
    "print(f\"  Subsample: {gb_param_grid['subsample']}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in gb_param_grid.values()])}\")\n",
    "\n",
    "# RandomizedSearchCV for efficiency\n",
    "gb_random = RandomizedSearchCV(\n",
    "    GradientBoostingClassifier(random_state=RANDOM_STATE),\n",
    "    gb_param_grid,\n",
    "    n_iter=40,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\nRunning RandomizedSearchCV (this may take a few minutes)...\")\n",
    "gb_random.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {gb_random.best_params_}\")\n",
    "print(f\"Best CV accuracy: {gb_random.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "gb_best = gb_random.best_estimator_\n",
    "y_pred_gb = gb_best.predict(X_test_scaled)\n",
    "y_proba_gb = gb_best.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "gb_metrics = calculate_metrics(y_test, y_pred_gb, y_proba_gb)\n",
    "\n",
    "# Calculate OOB improvement (staged predictions on training set)\n",
    "oob_improvement = gb_best.train_score_[-1] * 100\n",
    "gb_metrics['oob_improvement'] = oob_improvement\n",
    "\n",
    "print_metrics(gb_metrics, \"Gradient Boosting - Test Set Performance\")\n",
    "print(f\"Training score (OOB-like): {oob_improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: XGBoost\n",
    "\n",
    "Hyperparameter search for:\n",
    "- Max depth\n",
    "- Learning rate\n",
    "- Number of estimators\n",
    "- Subsample\n",
    "- Colsample by tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 5: XGBoost\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "xgb_param_grid = {\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "print(f\"\\nHyperparameter search space:\")\n",
    "print(f\"  Max depth: {xgb_param_grid['max_depth']}\")\n",
    "print(f\"  Learning rate: {xgb_param_grid['learning_rate']}\")\n",
    "print(f\"  N estimators: {xgb_param_grid['n_estimators']}\")\n",
    "print(f\"  Subsample: {xgb_param_grid['subsample']}\")\n",
    "print(f\"  Colsample by tree: {xgb_param_grid['colsample_bytree']}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) for v in xgb_param_grid.values()])}\")\n",
    "\n",
    "# RandomizedSearchCV for efficiency\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='logloss'),\n",
    "    xgb_param_grid,\n",
    "    n_iter=50,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"\\nRunning RandomizedSearchCV (this may take a few minutes)...\")\n",
    "xgb_random.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {xgb_random.best_params_}\")\n",
    "print(f\"Best CV accuracy: {xgb_random.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "xgb_best = xgb_random.best_estimator_\n",
    "y_pred_xgb = xgb_best.predict(X_test_scaled)\n",
    "y_proba_xgb = xgb_best.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "xgb_metrics = calculate_metrics(y_test, y_pred_xgb, y_proba_xgb)\n",
    "print_metrics(xgb_metrics, \"XGBoost - Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6: Logistic Regression\n",
    "\n",
    "Hyperparameter search for:\n",
    "- C (regularization parameter)\n",
    "- Penalty type (L1, L2, elastic net)\n",
    "- Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MODEL 6: LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "lr_param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [1000]\n",
    "}\n",
    "\n",
    "print(f\"\\nHyperparameter search space:\")\n",
    "print(f\"  C: {lr_param_grid['C']}\")\n",
    "print(f\"  Penalty: {lr_param_grid['penalty']}\")\n",
    "print(f\"  Solver: {lr_param_grid['solver']}\")\n",
    "print(f\"\\nTotal combinations: {np.prod([len(v) if isinstance(v, list) else 1 for v in lr_param_grid.values()])}\")\n",
    "\n",
    "# GridSearchCV with 5-fold cross-validation\n",
    "lr_grid = GridSearchCV(\n",
    "    LogisticRegression(random_state=RANDOM_STATE),\n",
    "    lr_param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nRunning GridSearchCV...\")\n",
    "lr_grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest parameters: {lr_grid.best_params_}\")\n",
    "print(f\"Best CV accuracy: {lr_grid.best_score_ * 100:.2f}%\")\n",
    "\n",
    "# Evaluate on test set\n",
    "lr_best = lr_grid.best_estimator_\n",
    "y_pred_lr = lr_best.predict(X_test_scaled)\n",
    "y_proba_lr = lr_best.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "lr_metrics = calculate_metrics(y_test, y_pred_lr, y_proba_lr)\n",
    "print_metrics(lr_metrics, \"Logistic Regression - Test Set Performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "comparison_data = [\n",
    "    {\n",
    "        'Model': 'Neural Network',\n",
    "        'Accuracy': f\"{nn_metrics['accuracy']:.2f}%\",\n",
    "        'Sensitivity': f\"{nn_metrics['sensitivity']:.2f}%\",\n",
    "        'Specificity': f\"{nn_metrics['specificity']:.2f}%\",\n",
    "        'Precision': f\"{nn_metrics['precision']:.2f}%\",\n",
    "        'F1 Score': f\"{nn_metrics['f1']:.2f}%\",\n",
    "        'AUC': f\"{nn_metrics['auc']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'SVM',\n",
    "        'Accuracy': f\"{svm_metrics['accuracy']:.2f}%\",\n",
    "        'Sensitivity': f\"{svm_metrics['sensitivity']:.2f}%\",\n",
    "        'Specificity': f\"{svm_metrics['specificity']:.2f}%\",\n",
    "        'Precision': f\"{svm_metrics['precision']:.2f}%\",\n",
    "        'F1 Score': f\"{svm_metrics['f1']:.2f}%\",\n",
    "        'AUC': f\"{svm_metrics['auc']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Random Forest',\n",
    "        'Accuracy': f\"{rf_metrics['accuracy']:.2f}%\",\n",
    "        'Sensitivity': f\"{rf_metrics['sensitivity']:.2f}%\",\n",
    "        'Specificity': f\"{rf_metrics['specificity']:.2f}%\",\n",
    "        'Precision': f\"{rf_metrics['precision']:.2f}%\",\n",
    "        'F1 Score': f\"{rf_metrics['f1']:.2f}%\",\n",
    "        'AUC': f\"{rf_metrics['auc']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Gradient Boosting',\n",
    "        'Accuracy': f\"{gb_metrics['accuracy']:.2f}%\",\n",
    "        'Sensitivity': f\"{gb_metrics['sensitivity']:.2f}%\",\n",
    "        'Specificity': f\"{gb_metrics['specificity']:.2f}%\",\n",
    "        'Precision': f\"{gb_metrics['precision']:.2f}%\",\n",
    "        'F1 Score': f\"{gb_metrics['f1']:.2f}%\",\n",
    "        'AUC': f\"{gb_metrics['auc']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'XGBoost',\n",
    "        'Accuracy': f\"{xgb_metrics['accuracy']:.2f}%\",\n",
    "        'Sensitivity': f\"{xgb_metrics['sensitivity']:.2f}%\",\n",
    "        'Specificity': f\"{xgb_metrics['specificity']:.2f}%\",\n",
    "        'Precision': f\"{xgb_metrics['precision']:.2f}%\",\n",
    "        'F1 Score': f\"{xgb_metrics['f1']:.2f}%\",\n",
    "        'AUC': f\"{xgb_metrics['auc']:.4f}\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Accuracy': f\"{lr_metrics['accuracy']:.2f}%\",\n",
    "        'Sensitivity': f\"{lr_metrics['sensitivity']:.2f}%\",\n",
    "        'Specificity': f\"{lr_metrics['specificity']:.2f}%\",\n",
    "        'Precision': f\"{lr_metrics['precision']:.2f}%\",\n",
    "        'F1 Score': f\"{lr_metrics['f1']:.2f}%\",\n",
    "        'AUC': f\"{lr_metrics['auc']:.4f}\"\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\" + \"=\" * 120)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON\")\n",
    "print(\"=\" * 120)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\" * 120)\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv('../scripts/ml_models_comparison.csv', index=False)\n",
    "print(\"\\nComparison saved to 'ml_models_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric values for plotting\n",
    "models = ['Neural\\nNetwork', 'SVM', 'Random\\nForest', 'Gradient\\nBoosting', 'XGBoost', 'Logistic\\nRegression']\n",
    "accuracy_vals = [nn_metrics['accuracy'], svm_metrics['accuracy'], rf_metrics['accuracy'], \n",
    "                 gb_metrics['accuracy'], xgb_metrics['accuracy'], lr_metrics['accuracy']]\n",
    "sensitivity_vals = [nn_metrics['sensitivity'], svm_metrics['sensitivity'], rf_metrics['sensitivity'],\n",
    "                   gb_metrics['sensitivity'], xgb_metrics['sensitivity'], lr_metrics['sensitivity']]\n",
    "specificity_vals = [nn_metrics['specificity'], svm_metrics['specificity'], rf_metrics['specificity'],\n",
    "                   gb_metrics['specificity'], xgb_metrics['specificity'], lr_metrics['specificity']]\n",
    "auc_vals = [nn_metrics['auc'], svm_metrics['auc'], rf_metrics['auc'],\n",
    "           gb_metrics['auc'], xgb_metrics['auc'], lr_metrics['auc']]\n",
    "\n",
    "# Create comprehensive comparison plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plot 1: Accuracy comparison\n",
    "axes[0, 0].bar(models, accuracy_vals, color='steelblue', alpha=0.8)\n",
    "axes[0, 0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0, 0].set_title('(a) Model Accuracy Comparison', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].set_ylim([0, 100])\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(accuracy_vals):\n",
    "    axes[0, 0].text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 2: Sensitivity vs Specificity\n",
    "x = np.arange(len(models))\n",
    "width = 0.35\n",
    "axes[0, 1].bar(x - width/2, sensitivity_vals, width, label='Sensitivity', color='coral', alpha=0.8)\n",
    "axes[0, 1].bar(x + width/2, specificity_vals, width, label='Specificity', color='lightgreen', alpha=0.8)\n",
    "axes[0, 1].set_ylabel('Score (%)', fontsize=12)\n",
    "axes[0, 1].set_title('(b) Sensitivity vs Specificity', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x)\n",
    "axes[0, 1].set_xticklabels(models)\n",
    "axes[0, 1].set_ylim([0, 100])\n",
    "axes[0, 1].legend(fontsize=11)\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: AUC comparison\n",
    "axes[1, 0].bar(models, [a * 100 for a in auc_vals], color='purple', alpha=0.7)\n",
    "axes[1, 0].set_ylabel('AUC Score (%)', fontsize=12)\n",
    "axes[1, 0].set_title('(c) AUC Score Comparison', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].set_ylim([0, 100])\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "for i, v in enumerate(auc_vals):\n",
    "    axes[1, 0].text(i, v * 100 + 1, f'{v:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# Plot 4: Overall performance radar (using accuracy, sensitivity, specificity, AUC)\n",
    "# We'll create a grouped bar chart instead for clarity\n",
    "metrics_names = ['Accuracy', 'Sensitivity', 'Specificity', 'AUC']\n",
    "x_pos = np.arange(len(metrics_names))\n",
    "bar_width = 0.13\n",
    "\n",
    "for i, (model, acc, sens, spec, auc) in enumerate(zip(\n",
    "    ['NN', 'SVM', 'RF', 'GB', 'XGB', 'LR'],\n",
    "    accuracy_vals, sensitivity_vals, specificity_vals, [a * 100 for a in auc_vals]\n",
    ")):\n",
    "    offset = (i - 2.5) * bar_width\n",
    "    axes[1, 1].bar(x_pos + offset, [acc, sens, spec, auc], bar_width, label=model)\n",
    "\n",
    "axes[1, 1].set_ylabel('Score (%)', fontsize=12)\n",
    "axes[1, 1].set_title('(d) Overall Performance Metrics', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].set_xticks(x_pos)\n",
    "axes[1, 1].set_xticklabels(metrics_names)\n",
    "axes[1, 1].legend(ncol=3, fontsize=9, loc='lower right')\n",
    "axes[1, 1].set_ylim([0, 100])\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../scripts/ml_models_comprehensive_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Comprehensive comparison plot saved as 'ml_models_comprehensive_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curves for all models\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Calculate ROC curves for each model\n",
    "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_proba_nn)\n",
    "fpr_svm, tpr_svm, _ = roc_curve(y_test, y_proba_svm)\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, y_proba_rf)\n",
    "fpr_gb, tpr_gb, _ = roc_curve(y_test, y_proba_gb)\n",
    "fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_proba_xgb)\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_proba_lr)\n",
    "\n",
    "# Plot all ROC curves\n",
    "plt.plot(fpr_nn, tpr_nn, label=f'Neural Network (AUC = {nn_metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "plt.plot(fpr_svm, tpr_svm, label=f'SVM (AUC = {svm_metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "plt.plot(fpr_gb, tpr_gb, label=f'Gradient Boosting (AUC = {gb_metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {xgb_metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {lr_metrics[\"auc\"]:.3f})', linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=2)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=12)\n",
    "plt.ylabel('True Positive Rate', fontsize=12)\n",
    "plt.title('ROC Curves - All Models Comparison', fontsize=14, fontweight='bold')\n",
    "plt.legend(loc=\"lower right\", fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../scripts/roc_curves_all_models.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"ROC curves saved as 'roc_curves_all_models.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Search Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of best hyperparameters for each model\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"BEST HYPERPARAMETERS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "hyperparam_summary = [\n",
    "    {\n",
    "        'Model': 'Neural Network',\n",
    "        'Best Parameters': str(nn_grid.best_params_),\n",
    "        'CV Accuracy': f\"{nn_grid.best_score_ * 100:.2f}%\",\n",
    "        'Test Accuracy': f\"{nn_metrics['accuracy']:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'SVM',\n",
    "        'Best Parameters': str(svm_grid.best_params_),\n",
    "        'CV Accuracy': f\"{svm_grid.best_score_ * 100:.2f}%\",\n",
    "        'Test Accuracy': f\"{svm_metrics['accuracy']:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Random Forest',\n",
    "        'Best Parameters': str(rf_random.best_params_),\n",
    "        'CV Accuracy': f\"{rf_random.best_score_ * 100:.2f}%\",\n",
    "        'Test Accuracy': f\"{rf_metrics['accuracy']:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Gradient Boosting',\n",
    "        'Best Parameters': str(gb_random.best_params_),\n",
    "        'CV Accuracy': f\"{gb_random.best_score_ * 100:.2f}%\",\n",
    "        'Test Accuracy': f\"{gb_metrics['accuracy']:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'XGBoost',\n",
    "        'Best Parameters': str(xgb_random.best_params_),\n",
    "        'CV Accuracy': f\"{xgb_random.best_score_ * 100:.2f}%\",\n",
    "        'Test Accuracy': f\"{xgb_metrics['accuracy']:.2f}%\"\n",
    "    },\n",
    "    {\n",
    "        'Model': 'Logistic Regression',\n",
    "        'Best Parameters': str(lr_grid.best_params_),\n",
    "        'CV Accuracy': f\"{lr_grid.best_score_ * 100:.2f}%\",\n",
    "        'Test Accuracy': f\"{lr_metrics['accuracy']:.2f}%\"\n",
    "    }\n",
    "]\n",
    "\n",
    "hyperparam_df = pd.DataFrame(hyperparam_summary)\n",
    "for _, row in hyperparam_df.iterrows():\n",
    "    print(f\"\\n{row['Model']}:\")\n",
    "    print(f\"  Best Parameters: {row['Best Parameters']}\")\n",
    "    print(f\"  CV Accuracy: {row['CV Accuracy']}\")\n",
    "    print(f\"  Test Accuracy: {row['Test Accuracy']}\")\n",
    "\n",
    "# Save to CSV\n",
    "hyperparam_df.to_csv('../scripts/hyperparameter_summary.csv', index=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Hyperparameter summary saved to 'hyperparameter_summary.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Recommendations\n",
    "\n",
    "This notebook has successfully implemented and compared six different machine learning models for fall prediction:\n",
    "\n",
    "1. **Neural Network (MLP)**: Deep learning approach with multiple hidden layers\n",
    "2. **SVM**: Support Vector Machine with RBF, linear, and polynomial kernels\n",
    "3. **Random Forest**: Ensemble method with OOB error estimation\n",
    "4. **Gradient Boosting**: Sequential ensemble with boosting\n",
    "5. **XGBoost**: Optimized gradient boosting implementation\n",
    "6. **Logistic Regression**: Linear baseline model\n",
    "\n",
    "### Methodology:\n",
    "- **Data Split**: 75/25 stratified train/test split to maintain class balance\n",
    "- **Hyperparameter Tuning**: GridSearchCV or RandomizedSearchCV with 5-fold cross-validation\n",
    "- **OOB Estimation**: Used for Random Forest and tracked for Gradient Boosting\n",
    "- **Metrics**: Comprehensive evaluation including accuracy, sensitivity, specificity, precision, F1, and AUC\n",
    "\n",
    "### Next Steps:\n",
    "1. Ensemble methods combining top models\n",
    "2. Feature importance analysis\n",
    "3. Error analysis and misclassification patterns\n",
    "4. Clinical validation with domain experts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
