
====================================================================================================
                 COMPREHENSIVE COMPARISON: Baseline (Soangra et al.) vs. Extensions                 
====================================================================================================

Started: 2025-11-10 01:49:59

================================================================================
Loading Data
================================================================================
Loaded data: 171 samples, 63 columns

Warning: Found 1 NaN values across 1 columns
NaN values filled with column medians
Features: 61 columns
Target distribution: Fallers=34, Non-Fallers=137

Train set: 128 samples (Fallers=25)
Test set: 43 samples (Fallers=9)

Data loaded: Train=128, Test=43
Features: 61

================================================================================
BASELINE: Replicating Soangra et al. (Nature 2021) - Experiment III
================================================================================

Step 1: PCA Feature Engineering (99% variance threshold)

Feature categorization:
  Linear features: 23
  Nonlinear features: 38

Linear features - stability testing:

  Feature selection results:
    Original features: 23
    Stable features: 23
    Removed (unstable): 0

Nonlinear features - stability testing:

  Feature selection results:
    Original features: 38
    Stable features: 38
    Removed (unstable): 0

Fitting PCA for linear features...
  Linear: 23 features → 13 PCs (variance: 0.993)

Fitting PCA for nonlinear features...
  Nonlinear: 38 features → 24 PCs (variance: 0.991)

PCA results:
  Linear PCs: 13
  Nonlinear PCs: 24

Using Soangra et al.'s configuration:
  4 linear PCs + 24 nonlinear PCs
  Final feature dimensions: 28 PCs

Step 2: Training Random Forest (Soangra et al. configuration)
  Architecture: 365 trees, max_features=1 (1 feature per split)
  Running 10 iterations with different random seeds...
    Seed 0: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.438, OOB=0.805
    Seed 1: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.374, OOB=0.805
    Seed 2: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.412, OOB=0.805
    Seed 3: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.358, OOB=0.805
    Seed 4: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.364, OOB=0.805
    Seed 5: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.373, OOB=0.805
    Seed 6: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.386, OOB=0.805
    Seed 7: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.391, OOB=0.805
    Seed 8: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.348, OOB=0.805
    Seed 9: Acc=0.791, Sens=0.000, Spec=1.000, AUC=0.338, OOB=0.805

--------------------------------------------------------------------------------
BASELINE RESULTS (Mean ± SE from 10 runs):
--------------------------------------------------------------------------------
ACCURACY       : 0.791 ± 0.000
SENSITIVITY    : 0.000 ± 0.000
SPECIFICITY    : 1.000 ± 0.000
PRECISION      : 0.000 ± 0.000
AUC_ROC        : 0.378 ± 0.028
OOB_SCORE      : 0.805 ± 0.000
--------------------------------------------------------------------------------

================================================================================
EXTENSION 1: PCA + Optimized Random Forest
================================================================================

Using PCA features: 4 linear + 24 nonlinear = 28 PCs

Optimizing RF hyperparameters via RandomizedSearchCV...
Fitting 5 folds for each of 50 candidates, totalling 250 fits

Best hyperparameters: {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 1, 'max_depth': None, 'class_weight': None}
Best CV AUC-ROC: 0.6495

================================================================================
EXTENSION 2: Raw Features + Multiple Algorithms
================================================================================

1. Random Forest (Raw Features, Tuned)...
   Best params: {'n_estimators': 300, 'min_samples_split': 10, 'max_features': None, 'max_depth': 30, 'class_weight': None}

2. Gradient Boosting...

3. XGBoost...

================================================================================
EXTENSION 3: PCA + SMOTE + Random Forest
================================================================================

Applying SMOTE to balance training data...
  Original: Fallers=25, Non-fallers=103
  After SMOTE: Fallers=103, Non-fallers=103

Training Random Forest on SMOTE-balanced data...

====================================================================================================
                             FINAL COMPARISON: Baseline vs. Extensions                              
====================================================================================================

               Model                              Approach      Accuracy   Sensitivity   Specificity       AUC-ROC   OOB      Features
    Soangra_Baseline PCA + Fixed RF (365trees, max_feat=1) 0.791 ± 0.000 0.000 ± 0.000 1.000 ± 0.000 0.378 ± 0.028 0.805 4L + 24NL PCs
        PCA_RF_Tuned                         Our Extension 0.793 ± 0.062 0.000 ± 0.000 1.000 ± 0.000 0.404 ± 0.114 0.805           PCA
        RF_Raw_Tuned                         Our Extension 0.700 ± 0.073 0.221 ± 0.145 0.824 ± 0.065 0.607 ± 0.104 0.773      Raw (61)
GradientBoosting_Raw                         Our Extension 0.698 ± 0.074 0.332 ± 0.164 0.794 ± 0.071 0.589 ± 0.116   N/A      Raw (61)
         XGBoost_Raw                         Our Extension 0.652 ± 0.073 0.000 ± 0.000 0.822 ± 0.065 0.559 ± 0.091   N/A      Raw (61)
        PCA_RF_SMOTE                         Our Extension 0.627 ± 0.074 0.000 ± 0.000 0.790 ± 0.069 0.393 ± 0.093 0.942           PCA

Results saved to: outputs/baseline_comparison/comparison_results.csv

====================================================================================================
Completed: 2025-11-10 01:51:57
====================================================================================================
